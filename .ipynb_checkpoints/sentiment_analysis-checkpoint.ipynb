{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents\n",
    "\n",
    "* Import and preprocess Assignment Data\n",
    "* Import and preprocess Labeled Twitter Data\n",
    "* Split/train/test Labeled Twitter data model\n",
    "* Split/train/test Movie Review data model\n",
    "* Compare results of both models on Labeled Twitter data Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import html.parser as HTMLParser# In Python 3.4+ import html \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>full_location</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri Aug 12 10:04:02 +0000 2016</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>en</td>\n",
       "      <td>Baton Rouge, LA</td>\n",
       "      <td>United States</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri Aug 12 10:04:30 +0000 2016</td>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>en</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>United States</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 12 10:04:46 +0000 2016</td>\n",
       "      <td>@realDonaldTrump, you wouldn't recognize a lie...</td>\n",
       "      <td>en</td>\n",
       "      <td>Palm Springs, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri Aug 12 10:04:48 +0000 2016</td>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Secaucus, NJ</td>\n",
       "      <td>United States</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Aug 12 10:04:48 +0000 2016</td>\n",
       "      <td>@HillaryClinton you ARE the co-founder of ISIS...</td>\n",
       "      <td>en</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>United States</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Fri Aug 12 10:04:02 +0000 2016   \n",
       "1  Fri Aug 12 10:04:30 +0000 2016   \n",
       "2  Fri Aug 12 10:04:46 +0000 2016   \n",
       "3  Fri Aug 12 10:04:48 +0000 2016   \n",
       "4  Fri Aug 12 10:04:48 +0000 2016   \n",
       "\n",
       "                                                text lang     full_location  \\\n",
       "0  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   en   Baton Rouge, LA   \n",
       "1  #CNN #newday clear #Trump deliberately throwin...   en     Baltimore, MD   \n",
       "2  @realDonaldTrump, you wouldn't recognize a lie...   en  Palm Springs, CA   \n",
       "3  \"Kid, you know, suing someone? Thats the most ...   en      Secaucus, NJ   \n",
       "4  @HillaryClinton you ARE the co-founder of ISIS...   en        Irving, TX   \n",
       "\n",
       "         country state  \n",
       "0  United States    LA  \n",
       "1  United States    MD  \n",
       "2  United States    CA  \n",
       "3  United States    NJ  \n",
       "4  United States    TX  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_feather('data/tweets_by_state.feather')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[[0]]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract then Remove Hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# A function that extracts the hyperlinks from the tweet's content.\n",
    "def extract_link(text):\n",
    "    regex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(regex, text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return ''\n",
    "\n",
    "# A function that removes the hyperlink and tokenizes the text\n",
    "def clean_text(text):\n",
    "    regex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(regex, text)\n",
    "    if match: \n",
    "        result = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return tokenizer.tokenize(result.lower())\n",
    "    return tokenizer.tokenize(text.lower())\n",
    "\n",
    "# A function that checks whether a word is included in the tweet's content\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['link'] = tweets['text'].apply(lambda tweet: extract_link(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_tokens'] = tweets['text'].apply(lambda tweet: clean_text(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>full_location</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>link</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri Aug 12 10:04:02 +0000 2016</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>en</td>\n",
       "      <td>Baton Rouge, LA</td>\n",
       "      <td>United States</td>\n",
       "      <td>LA</td>\n",
       "      <td>https://t.co/5GMNZq40V3</td>\n",
       "      <td>[barackobama, fbi, lorettalynch, all, in, coll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri Aug 12 10:04:30 +0000 2016</td>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>en</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>United States</td>\n",
       "      <td>MD</td>\n",
       "      <td></td>\n",
       "      <td>[cnn, newday, clear, trump, deliberately, thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 12 10:04:46 +0000 2016</td>\n",
       "      <td>@realDonaldTrump, you wouldn't recognize a lie...</td>\n",
       "      <td>en</td>\n",
       "      <td>Palm Springs, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>CA</td>\n",
       "      <td>https://t.co/pKSQM8yikm</td>\n",
       "      <td>[realdonaldtrump, you, wouldn, t, recognize, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri Aug 12 10:04:48 +0000 2016</td>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Secaucus, NJ</td>\n",
       "      <td>United States</td>\n",
       "      <td>NJ</td>\n",
       "      <td></td>\n",
       "      <td>[kid, you, know, suing, someone, thats, the, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Aug 12 10:04:48 +0000 2016</td>\n",
       "      <td>@HillaryClinton you ARE the co-founder of ISIS...</td>\n",
       "      <td>en</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>United States</td>\n",
       "      <td>TX</td>\n",
       "      <td></td>\n",
       "      <td>[hillaryclinton, you, are, the, co, founder, o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Fri Aug 12 10:04:02 +0000 2016   \n",
       "1  Fri Aug 12 10:04:30 +0000 2016   \n",
       "2  Fri Aug 12 10:04:46 +0000 2016   \n",
       "3  Fri Aug 12 10:04:48 +0000 2016   \n",
       "4  Fri Aug 12 10:04:48 +0000 2016   \n",
       "\n",
       "                                                text lang     full_location  \\\n",
       "0  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   en   Baton Rouge, LA   \n",
       "1  #CNN #newday clear #Trump deliberately throwin...   en     Baltimore, MD   \n",
       "2  @realDonaldTrump, you wouldn't recognize a lie...   en  Palm Springs, CA   \n",
       "3  \"Kid, you know, suing someone? Thats the most ...   en      Secaucus, NJ   \n",
       "4  @HillaryClinton you ARE the co-founder of ISIS...   en        Irving, TX   \n",
       "\n",
       "         country state                     link  \\\n",
       "0  United States    LA  https://t.co/5GMNZq40V3   \n",
       "1  United States    MD                            \n",
       "2  United States    CA  https://t.co/pKSQM8yikm   \n",
       "3  United States    NJ                            \n",
       "4  United States    TX                            \n",
       "\n",
       "                                        clean_tokens  \n",
       "0  [barackobama, fbi, lorettalynch, all, in, coll...  \n",
       "1  [cnn, newday, clear, trump, deliberately, thro...  \n",
       "2  [realdonaldtrump, you, wouldn, t, recognize, a...  \n",
       "3  [kid, you, know, suing, someone, thats, the, m...  \n",
       "4  [hillaryclinton, you, are, the, co, founder, o...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Labeled Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets = pd.read_csv('data/raw_data/training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\", usecols=[0,5], names=['sentiment', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets.loc[labeled_tweets['sentiment'] == 4, 'sentiment'] = 'pos'\n",
    "labeled_tweets.loc[labeled_tweets['sentiment'] == 0, 'sentiment'] = 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0       neg  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       neg  is upset that he can't update his Facebook by ...\n",
       "2       neg  @Kenichan I dived many times for the ball. Man...\n",
       "3       neg    my whole body feels itchy and like its on fire \n",
       "4       neg  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>pos</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>pos</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>pos</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>pos</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>pos</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                               text\n",
       "1599995       pos  Just woke up. Having no school is the best fee...\n",
       "1599996       pos  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997       pos  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998       pos  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999       pos  happy #charitytuesday @theNSPCC @SparksCharity..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    800000\n",
       "pos    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "def tokenize_tweets(text):\n",
    "    regex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(regex, text)\n",
    "    if match: \n",
    "        result = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return tokenizer.tokenize(result.lower())\n",
    "    return tokenizer.tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets['clean_tokens'] = labeled_tweets['text'].apply(lambda tweet: tokenize_tweets(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets['clean_tokens'] = labeled_tweets['clean_tokens'].apply(lambda x: [item for item in x if item not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[-, awww, ,, that's, bummer, ., shoulda, got, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, can't, update, facebook, texting, ...,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[dived, many, times, ball, ., managed, save, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[,, behaving, ., i'm, mad, ., ?, can't, see, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0       neg  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1       neg  is upset that he can't update his Facebook by ...   \n",
       "2       neg  @Kenichan I dived many times for the ball. Man...   \n",
       "3       neg    my whole body feels itchy and like its on fire    \n",
       "4       neg  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                        clean_tokens  \n",
       "0  [-, awww, ,, that's, bummer, ., shoulda, got, ...  \n",
       "1  [upset, can't, update, facebook, texting, ...,...  \n",
       "2  [dived, many, times, ball, ., managed, save, 5...  \n",
       "3            [whole, body, feels, itchy, like, fire]  \n",
       "4    [,, behaving, ., i'm, mad, ., ?, can't, see, .]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (End preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['love', 'u', 'guys', 'r', 'best', '!', '!'], 'pos'),\n",
      " (['im',\n",
      "   'meeting',\n",
      "   'one',\n",
      "   'besties',\n",
      "   'tonight',\n",
      "   '!',\n",
      "   'cant',\n",
      "   'wait',\n",
      "   '!',\n",
      "   '!',\n",
      "   '-',\n",
      "   'girl',\n",
      "   'talk',\n",
      "   '!',\n",
      "   '!'],\n",
      "  'pos')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pos_tweets_df = labeled_tweets[labeled_tweets['sentiment']=='pos']\n",
    "pos_tweets = []\n",
    "\n",
    "def feat_format(token):\n",
    "    pos_tweets.append((token,'pos'))\n",
    "\n",
    "pos_tweets_df['clean_tokens'].apply(lambda token: feat_format(token))\n",
    "pprint(pos_tweets[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['-',\n",
      "   'awww',\n",
      "   ',',\n",
      "   \"that's\",\n",
      "   'bummer',\n",
      "   '.',\n",
      "   'shoulda',\n",
      "   'got',\n",
      "   'david',\n",
      "   'carr',\n",
      "   'third',\n",
      "   'day',\n",
      "   '.',\n",
      "   ';d'],\n",
      "  'neg'),\n",
      " (['upset',\n",
      "   \"can't\",\n",
      "   'update',\n",
      "   'facebook',\n",
      "   'texting',\n",
      "   '...',\n",
      "   'might',\n",
      "   'cry',\n",
      "   'result',\n",
      "   'school',\n",
      "   'today',\n",
      "   'also',\n",
      "   '.',\n",
      "   'blah',\n",
      "   '!'],\n",
      "  'neg')]\n"
     ]
    }
   ],
   "source": [
    "neg_tweets_df = labeled_tweets[labeled_tweets['sentiment']=='neg']\n",
    "neg_tweets = []\n",
    "\n",
    "def feat_format(token):\n",
    "    neg_tweets.append((token,'neg'))\n",
    "\n",
    "neg_tweets_df['clean_tokens'].apply(lambda token: feat_format(token))\n",
    "pprint(neg_tweets[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Reduce data set\n",
    "\n",
    "Accuracy improves with larger dataset, but takes exponentially more time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = pos_tweets[:int((len(pos_tweets)*0.001))]\n",
    "neg_tweets = neg_tweets[:int((len(neg_tweets)*0.001))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_tweets+neg_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract List of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the separate words in tweets\n",
    "# Input:  A list of tweets\n",
    "# Output: A list of all words in the tweets\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "# Create a dictionary measuring word frequencies\n",
    "# Input: the list of words\n",
    "# Output: the frequency of those words apearing in tweets\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    print (\"Word frequency list created\\n\")\n",
    "    # pprint(type(wordlist))\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_features = get_word_features(get_words_in_tweets(pos_tweets + neg_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "negcutoff = int(len(neg_tweets)*3/4)\n",
    "poscutoff = int(len(pos_tweets)*3/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = neg_tweets[:negcutoff] + pos_tweets[:poscutoff]\n",
    "test_tweets = neg_tweets[negcutoff:] + pos_tweets[poscutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequency list created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_features = get_word_features(get_words_in_tweets(train_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our features based on which tweets contain which word\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we apply the features we constructed to our tweets data.\n",
    "training_set = nltk.classify.apply_features(extract_features, train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the resulting training set shows the features we are going to pass to the classifier.\n",
    "# pprint(training_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7127"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the line of code that we use to train our classifier. Training is performed in a streamlined way so no output is visible.\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6716666666666666\n"
     ]
    }
   ],
   "source": [
    "test_set = nltk.classify.apply_features(extract_features,test_tweets)\n",
    "print ('accuracy:', nltk.classify.util.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           contains(sad) = True              neg : pos    =     12.4 : 1.0\n",
      "          contains(sick) = True              neg : pos    =     11.6 : 1.0\n",
      "          contains(blog) = True              pos : neg    =     10.3 : 1.0\n",
      "          contains(poor) = True              neg : pos    =      9.8 : 1.0\n",
      "         contains(sucks) = True              neg : pos    =      9.0 : 1.0\n",
      "        contains(missed) = True              neg : pos    =      8.4 : 1.0\n",
      "          contains(woke) = True              neg : pos    =      8.3 : 1.0\n",
      "           contains(cry) = True              neg : pos    =      7.7 : 1.0\n",
      "      contains(headache) = True              neg : pos    =      7.7 : 1.0\n",
      "         contains(heard) = True              neg : pos    =      7.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This snippet downloads the most popular datasets for experimenting with NLTK functionalities.\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "\n",
    "# A function that extracts which words exist in a text based on a list of words to which we compare.\n",
    "def word_feats(words):\n",
    "        return dict([(word, True) for word in words])\n",
    "\n",
    "# Get the negative reviews for movies    \n",
    "negids = movie_reviews.fileids('neg')\n",
    "\n",
    "# Get the positive reviews for movies\n",
    "posids = movie_reviews.fileids('pos')\n",
    " \n",
    "# Find the features that most correspond to negative reviews    \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "\n",
    "# Find the features that most correspond to positive reviews\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# We would only use 1500 instances to train on. The quarter of the reviews left is for testing purposes.\n",
    "negcutoff = int(len(negfeats)*3/4)\n",
    "poscutoff = int(len(posfeats)*3/4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Construct the training dataset containing 50% positive reviews and 50% negative reviews\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "\n",
    "# Construct the negative dataset containing 50% positive reviews and 50% negative reviews\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "print ('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train a NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "\n",
    "# Test the trained classifier and display the most informative features.\n",
    "print ('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MovieDataModel on Twitter_test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "print ('accuracy:', nltk.classify.util.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5412918cb870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_set' is not defined"
     ]
    }
   ],
   "source": [
    "test_set[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainfeats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0ce0163817b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainfeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainfeats' is not defined"
     ]
    }
   ],
   "source": [
    "trainfeats[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
