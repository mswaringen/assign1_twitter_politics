{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "\n",
    "* Import and preprocess Assignment Data\n",
    "* Import and preprocess Labeled Twitter Data\n",
    "* Split/train/test Labeled Twitter data model\n",
    "* Split/train/test Movie Review data model\n",
    "* Compare results of both models on Labeled Twitter data Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import html.parser as HTMLParser# In Python 3.4+ import html \n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "import collections, itertools\n",
    "import nltk.classify.util\n",
    "import nltk.metrics\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import precision\n",
    "from nltk import recall\n",
    "from nltk import BigramAssocMeasures\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Assignment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>full_location</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri Aug 12 10:04:02 +0000 2016</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>en</td>\n",
       "      <td>Baton Rouge, LA</td>\n",
       "      <td>United States</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri Aug 12 10:04:30 +0000 2016</td>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>en</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>United States</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 12 10:04:46 +0000 2016</td>\n",
       "      <td>@realDonaldTrump, you wouldn't recognize a lie...</td>\n",
       "      <td>en</td>\n",
       "      <td>Palm Springs, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri Aug 12 10:04:48 +0000 2016</td>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Secaucus, NJ</td>\n",
       "      <td>United States</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Aug 12 10:04:48 +0000 2016</td>\n",
       "      <td>@HillaryClinton you ARE the co-founder of ISIS...</td>\n",
       "      <td>en</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>United States</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Fri Aug 12 10:04:02 +0000 2016   \n",
       "1  Fri Aug 12 10:04:30 +0000 2016   \n",
       "2  Fri Aug 12 10:04:46 +0000 2016   \n",
       "3  Fri Aug 12 10:04:48 +0000 2016   \n",
       "4  Fri Aug 12 10:04:48 +0000 2016   \n",
       "\n",
       "                                                text lang     full_location  \\\n",
       "0  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   en   Baton Rouge, LA   \n",
       "1  #CNN #newday clear #Trump deliberately throwin...   en     Baltimore, MD   \n",
       "2  @realDonaldTrump, you wouldn't recognize a lie...   en  Palm Springs, CA   \n",
       "3  \"Kid, you know, suing someone? Thats the most ...   en      Secaucus, NJ   \n",
       "4  @HillaryClinton you ARE the co-founder of ISIS...   en        Irving, TX   \n",
       "\n",
       "         country state  \n",
       "0  United States    LA  \n",
       "1  United States    MD  \n",
       "2  United States    CA  \n",
       "3  United States    NJ  \n",
       "4  United States    TX  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_feather('data/tweets_by_state.feather')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[[0]]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract then Remove Hyperlinks, Tokenize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "def tokenize_tweets(text):\n",
    "    regex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(regex, text)\n",
    "    if match: \n",
    "        result = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return tokenizer.tokenize(result.lower())\n",
    "    return tokenizer.tokenize(text.lower())\n",
    "\n",
    "# A function that extracts the hyperlinks from the tweet's content.\n",
    "def extract_link(text):\n",
    "    regex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(regex, text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return ''\n",
    "\n",
    "# A function that checks whether a word is included in the tweet's content\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['link'] = tweets['text'].apply(lambda tweet: extract_link(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_tokens'] = tweets['text'].apply(lambda tweet: tokenize_tweets(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>full_location</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>link</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri Aug 12 10:04:02 +0000 2016</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>en</td>\n",
       "      <td>Baton Rouge, LA</td>\n",
       "      <td>United States</td>\n",
       "      <td>LA</td>\n",
       "      <td>https://t.co/5GMNZq40V3</td>\n",
       "      <td>[all, in, collusion, together, #nojustice, #tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri Aug 12 10:04:30 +0000 2016</td>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>en</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>United States</td>\n",
       "      <td>MD</td>\n",
       "      <td></td>\n",
       "      <td>[#cnn, #newday, clear, #trump, deliberately, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 12 10:04:46 +0000 2016</td>\n",
       "      <td>@realDonaldTrump, you wouldn't recognize a lie...</td>\n",
       "      <td>en</td>\n",
       "      <td>Palm Springs, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>CA</td>\n",
       "      <td>https://t.co/pKSQM8yikm</td>\n",
       "      <td>[,, you, wouldn't, recognize, a, lie, if, it, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri Aug 12 10:04:48 +0000 2016</td>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Secaucus, NJ</td>\n",
       "      <td>United States</td>\n",
       "      <td>NJ</td>\n",
       "      <td></td>\n",
       "      <td>[\", kid, ,, you, know, ,, suing, someone, ?, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Aug 12 10:04:48 +0000 2016</td>\n",
       "      <td>@HillaryClinton you ARE the co-founder of ISIS...</td>\n",
       "      <td>en</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>United States</td>\n",
       "      <td>TX</td>\n",
       "      <td></td>\n",
       "      <td>[you, are, the, co-founder, of, isis, ,, you, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Fri Aug 12 10:04:02 +0000 2016   \n",
       "1  Fri Aug 12 10:04:30 +0000 2016   \n",
       "2  Fri Aug 12 10:04:46 +0000 2016   \n",
       "3  Fri Aug 12 10:04:48 +0000 2016   \n",
       "4  Fri Aug 12 10:04:48 +0000 2016   \n",
       "\n",
       "                                                text lang     full_location  \\\n",
       "0  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   en   Baton Rouge, LA   \n",
       "1  #CNN #newday clear #Trump deliberately throwin...   en     Baltimore, MD   \n",
       "2  @realDonaldTrump, you wouldn't recognize a lie...   en  Palm Springs, CA   \n",
       "3  \"Kid, you know, suing someone? Thats the most ...   en      Secaucus, NJ   \n",
       "4  @HillaryClinton you ARE the co-founder of ISIS...   en        Irving, TX   \n",
       "\n",
       "         country state                     link  \\\n",
       "0  United States    LA  https://t.co/5GMNZq40V3   \n",
       "1  United States    MD                            \n",
       "2  United States    CA  https://t.co/pKSQM8yikm   \n",
       "3  United States    NJ                            \n",
       "4  United States    TX                            \n",
       "\n",
       "                                        clean_tokens  \n",
       "0  [all, in, collusion, together, #nojustice, #tr...  \n",
       "1  [#cnn, #newday, clear, #trump, deliberately, t...  \n",
       "2  [,, you, wouldn't, recognize, a, lie, if, it, ...  \n",
       "3  [\", kid, ,, you, know, ,, suing, someone, ?, t...  \n",
       "4  [you, are, the, co-founder, of, isis, ,, you, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Labeled Tweets Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets = pd.read_csv('data/raw_data/training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\", usecols=[0,5], names=['sentiment', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets.loc[labeled_tweets['sentiment'] == 4, 'sentiment'] = 'pos'\n",
    "labeled_tweets.loc[labeled_tweets['sentiment'] == 0, 'sentiment'] = 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0       neg  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       neg  is upset that he can't update his Facebook by ...\n",
       "2       neg  @Kenichan I dived many times for the ball. Man...\n",
       "3       neg    my whole body feels itchy and like its on fire \n",
       "4       neg  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>pos</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>pos</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>pos</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>pos</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>pos</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                               text\n",
       "1599995       pos  Just woke up. Having no school is the best fee...\n",
       "1599996       pos  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997       pos  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998       pos  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999       pos  happy #charitytuesday @theNSPCC @SparksCharity..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    800000\n",
       "neg    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Tweets & Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets['clean_tokens'] = labeled_tweets['text'].apply(lambda tweet: tokenize_tweets(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets['clean_tokens'] = labeled_tweets['clean_tokens'].apply(lambda x: [item for item in x if item not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets['clean_tokens'] = labeled_tweets['clean_tokens'].apply(lambda x: [item for item in x if len(item) >= 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[awww, that's, bummer, shoulda, got, david, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, can't, update, facebook, texting, ...,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[dived, many, times, ball, managed, save, rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, i'm, mad, can't, see]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0       neg  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1       neg  is upset that he can't update his Facebook by ...   \n",
       "2       neg  @Kenichan I dived many times for the ball. Man...   \n",
       "3       neg    my whole body feels itchy and like its on fire    \n",
       "4       neg  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                        clean_tokens  \n",
       "0  [awww, that's, bummer, shoulda, got, david, ca...  \n",
       "1  [upset, can't, update, facebook, texting, ...,...  \n",
       "2  [dived, many, times, ball, managed, save, rest...  \n",
       "3            [whole, body, feels, itchy, like, fire]  \n",
       "4                   [behaving, i'm, mad, can't, see]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (End preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Test Labeled Twitter Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting tokens and label into a list for pos and neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000     None\n",
       "800001     None\n",
       "800002     None\n",
       "800003     None\n",
       "800004     None\n",
       "           ... \n",
       "1599995    None\n",
       "1599996    None\n",
       "1599997    None\n",
       "1599998    None\n",
       "1599999    None\n",
       "Name: clean_tokens, Length: 800000, dtype: object"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pos_tweets_df = labeled_tweets[labeled_tweets['sentiment']=='pos']\n",
    "pos_tweets = []\n",
    "\n",
    "def feat_format(token):\n",
    "    pos_tweets.append((token,'pos'))\n",
    "\n",
    "pos_tweets_df['clean_tokens'].apply(lambda token: feat_format(token))\n",
    "# pprint(pos_tweets[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         None\n",
       "1         None\n",
       "2         None\n",
       "3         None\n",
       "4         None\n",
       "          ... \n",
       "799995    None\n",
       "799996    None\n",
       "799997    None\n",
       "799998    None\n",
       "799999    None\n",
       "Name: clean_tokens, Length: 800000, dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tweets_df = labeled_tweets[labeled_tweets['sentiment']=='neg']\n",
    "neg_tweets = []\n",
    "\n",
    "def feat_format(token):\n",
    "    neg_tweets.append((token,'neg'))\n",
    "\n",
    "neg_tweets_df['clean_tokens'].apply(lambda token: feat_format(token))\n",
    "# pprint(neg_tweets[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sample data set\n",
    "\n",
    "Accuracy improves with larger dataset, but takes exponentially more time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tweets: 1800\n",
      "Test Tweets: 600\n",
      "Total # of Tweets: 2400\n",
      "First tweet: (['cousinns', 'love', 'herrr', 'misss', 'tuggerah'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "subset_size = 0.0015\n",
    "pos_tweets = random.sample(pos_tweets,int(len(pos_tweets)*subset_size))\n",
    "neg_tweets = random.sample(neg_tweets,int(len(neg_tweets)*subset_size))\n",
    "negcutoff = int(len(neg_tweets)*3/4)\n",
    "poscutoff = int(len(pos_tweets)*3/4)\n",
    "\n",
    "train_tweets = neg_tweets[:negcutoff] + pos_tweets[:poscutoff]\n",
    "test_tweets = neg_tweets[negcutoff:] + pos_tweets[poscutoff:]\n",
    "\n",
    "print ('Training Tweets:', len(train_tweets))\n",
    "print ('Test Tweets:', len(test_tweets))\n",
    "print ('Total # of Tweets:',len(pos_tweets + neg_tweets))\n",
    "print ('First tweet:',train_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract List of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the separate words in tweets\n",
    "# Input:  A list of tweets\n",
    "# Output: A list of all words in the tweets\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "# Create a dictionary measuring word frequencies\n",
    "# Input: the list of words\n",
    "# Output: the frequency of those words apearing in tweets\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "#     print (\"Word frequency list created\\n\")\n",
    "    # pprint(type(wordlist))\n",
    "    return word_features\n",
    "\n",
    "# Construct our features based on which tweets contain which word\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "def extract_best(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in best_features:\n",
    "        features[word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "def find_best(feat_count):\n",
    "    word_fd = FreqDist()\n",
    "    label_word_fd = ConditionalFreqDist()\n",
    "    junk = ['p','o','s','n','e','g','...']\n",
    "\n",
    "    for tpl in pos_tweets[:poscutoff]:\n",
    "        for lst in tpl:\n",
    "            for word in lst:\n",
    "                if word not in junk:\n",
    "                    word_fd[word.lower()] += 1\n",
    "                    label_word_fd['pos'][word.lower()] += 1\n",
    "\n",
    "    for tpl in neg_tweets[:negcutoff]:\n",
    "        for lst in tpl:\n",
    "            for word in lst:\n",
    "                if word not in junk:\n",
    "                    word_fd[word.lower()] += 1\n",
    "                    label_word_fd['neg'][word.lower()] += 1\n",
    "\n",
    "    pos_word_count = len(label_word_fd['pos'])\n",
    "    neg_word_count = len(label_word_fd['neg'])\n",
    "    total_word_count = pos_word_count + neg_word_count\n",
    "\n",
    "    word_scores = {}\n",
    "\n",
    "    for word, freq in word_fd.items():\n",
    "        pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "            (freq, pos_word_count), total_word_count)\n",
    "        neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "            (freq, neg_word_count), total_word_count)\n",
    "        word_scores[word] = pos_score + neg_score\n",
    "\n",
    "    best = sorted(word_scores.items(), key=lambda s: s[1], reverse=True)[:feat_count]\n",
    "    bestwords = set([w for w, s in best])\n",
    "    return dict.fromkeys(bestwords,0).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_classifier(feat):\n",
    "    twitter_training_set = nltk.classify.apply_features(feat, train_tweets)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(twitter_training_set)\n",
    "\n",
    "    twitter_test_set = nltk.classify.apply_features(feat,test_tweets)\n",
    "\n",
    "    print ('train on %d instances, test on %d instances' % (len(twitter_training_set), len(twitter_test_set)))\n",
    "    print ('accuracy:', nltk.classify.util.accuracy(classifier, twitter_test_set))\n",
    "    classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Words as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_count: 4362\n",
      "train on 1800 instances, test on 600 instances\n",
      "accuracy: 0.705\n",
      "Most Informative Features\n",
      "                     sad = True              neg : pos    =     26.3 : 1.0\n",
      "                    sick = True              neg : pos    =     11.0 : 1.0\n",
      "                     hot = True              neg : pos    =      9.0 : 1.0\n",
      "                    cold = True              neg : pos    =      8.3 : 1.0\n",
      "                   bored = True              neg : pos    =      7.7 : 1.0\n",
      "                     job = True              neg : pos    =      7.7 : 1.0\n",
      "                    keep = True              pos : neg    =      7.7 : 1.0\n",
      "                    look = True              pos : neg    =      7.7 : 1.0\n",
      "                   thank = True              pos : neg    =      7.7 : 1.0\n",
      "                    wish = True              neg : pos    =      7.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "word_features = get_word_features(get_words_in_tweets(train_tweets))\n",
    "\n",
    "print ('feature_count:', len(word_features))\n",
    "twitter_classifier(extract_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Words as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_count: 100\n",
      "train on 1800 instances, test on 600 instances\n",
      "accuracy: 0.6883333333333334\n",
      "Most Informative Features\n",
      "                     sad = True              neg : pos    =     26.3 : 1.0\n",
      "                    sick = True              neg : pos    =     11.0 : 1.0\n",
      "                     hot = True              neg : pos    =      9.0 : 1.0\n",
      "                    cold = True              neg : pos    =      8.3 : 1.0\n",
      "                   bored = True              neg : pos    =      7.7 : 1.0\n",
      "                     job = True              neg : pos    =      7.7 : 1.0\n",
      "                    keep = True              pos : neg    =      7.7 : 1.0\n",
      "                    look = True              pos : neg    =      7.7 : 1.0\n",
      "                   thank = True              pos : neg    =      7.7 : 1.0\n",
      "                    wish = True              neg : pos    =      7.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Set desired feature count (ranked by score)\n",
    "feat_count = 100\n",
    "best_features = find_best(feat_count)\n",
    "\n",
    "print ('feature_count:', len(best_features))\n",
    "twitter_classifier(extract_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Feature Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_classifier(feat):\n",
    "    twitter_training_set = nltk.classify.apply_features(feat, train_tweets)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(twitter_training_set)\n",
    "    twitter_test_set = nltk.classify.apply_features(feat,test_tweets)\n",
    "    print ('feature_count / accuracy:', len(best_features), nltk.classify.util.accuracy(classifier, twitter_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_count / accuracy: 10 0.61\n",
      "feature_count / accuracy: 160 0.6816666666666666\n",
      "feature_count / accuracy: 310 0.6933333333333334\n",
      "feature_count / accuracy: 460 0.7016666666666667\n",
      "feature_count / accuracy: 610 0.695\n",
      "feature_count / accuracy: 760 0.7116666666666667\n",
      "feature_count / accuracy: 910 0.7166666666666667\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,1000,150):\n",
    "    feat_count = i\n",
    "    best_features = find_best(feat_count)\n",
    "    quick_classifier(extract_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_count / accuracy: 1000 0.7133333333333334\n",
      "feature_count / accuracy: 1250 0.7216666666666667\n",
      "feature_count / accuracy: 1500 0.7183333333333334\n",
      "feature_count / accuracy: 1750 0.695\n",
      "feature_count / accuracy: 2000 0.6766666666666666\n",
      "feature_count / accuracy: 2250 0.665\n",
      "feature_count / accuracy: 2500 0.69\n",
      "feature_count / accuracy: 2750 0.7116666666666667\n",
      "feature_count / accuracy: 3000 0.7216666666666667\n",
      "feature_count / accuracy: 3250 0.7116666666666667\n",
      "feature_count / accuracy: 3500 0.7166666666666667\n",
      "feature_count / accuracy: 3750 0.6983333333333334\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000,4000,250):\n",
    "    feat_count = i\n",
    "    best_features = find_best(feat_count)\n",
    "    quick_classifier(extract_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_count: 500\n",
      "train on 600 instances, test on 1800 instances\n",
      "accuracy: 0.6922222222222222\n",
      "Most Informative Features\n",
      "                     sad = True              neg : pos    =      7.7 : 1.0\n",
      "                 missing = True              neg : pos    =      7.0 : 1.0\n",
      "                    work = True              neg : pos    =      6.6 : 1.0\n",
      "                    love = True              pos : neg    =      5.4 : 1.0\n",
      "                    sick = True              neg : pos    =      5.0 : 1.0\n",
      "                   sorry = True              neg : pos    =      5.0 : 1.0\n",
      "                    hate = True              neg : pos    =      4.3 : 1.0\n",
      "                    miss = True              neg : pos    =      4.2 : 1.0\n",
      "                    well = True              pos : neg    =      3.9 : 1.0\n",
      "                  thanks = True              pos : neg    =      3.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "tmp = train_tweets\n",
    "train_tweets = test_tweets\n",
    "test_tweets = tmp\n",
    "\n",
    "feat_count = 500\n",
    "best_features = find_best(feat_count)\n",
    "\n",
    "print ('feature_count:', len(best_features))\n",
    "twitter_classifier(extract_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/mark/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This snippet downloads the most popular datasets for experimenting with NLTK functionalities.\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_classifier(feats):\n",
    "    # Get the negative reviews for movies    \n",
    "    negids = movie_reviews.fileids('neg')\n",
    "\n",
    "    # Get the positive reviews for movies\n",
    "    posids = movie_reviews.fileids('pos')\n",
    "\n",
    "    # Find the features that most correspond to negative reviews    \n",
    "    negfeats = [(feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "\n",
    "    # Find the features that most correspond to positive reviews\n",
    "    posfeats = [(feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "    # We would only use 1500 instances to train on. The quarter of the reviews left is for testing purposes.\n",
    "    negcutoff = int(len(negfeats)*3/4)\n",
    "    poscutoff = int(len(posfeats)*3/4)\n",
    "\n",
    "    # Construct the training dataset containing 50% positive reviews and 50% negative reviews\n",
    "    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "\n",
    "    # Construct the negative dataset containing 50% positive reviews and 50% negative reviews\n",
    "    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "    print ('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "    # Train a NaiveBayesClassifier\n",
    "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "\n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    "\n",
    "    print ('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "    print ('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "    print ('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "    print ('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "    print ('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "    classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Word Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that extracts which words exist in a text based on a list of words to which we compare.\n",
    "def word_feats(words):\n",
    "        return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating single word features\n",
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "pos precision: 0.651595744680851\n",
      "pos recall: 0.98\n",
      "neg precision: 0.9596774193548387\n",
      "neg recall: 0.476\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print ('evaluating single word features')\n",
    "eval_classifier(word_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Word Features using Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating best word features\n",
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.932\n",
      "pos precision: 0.8941605839416058\n",
      "pos recall: 0.98\n",
      "neg precision: 0.9778761061946902\n",
      "neg recall: 0.884\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist()\n",
    " \n",
    "for word in movie_reviews.words(categories=['pos']):\n",
    "    word_fd[word.lower()] += 1\n",
    "    label_word_fd['pos'][word.lower()] += 1\n",
    " \n",
    "for word in movie_reviews.words(categories=['neg']):\n",
    "    word_fd[word.lower()] += 1\n",
    "    label_word_fd['neg'][word.lower()] += 1\n",
    " \n",
    "pos_word_count = label_word_fd['pos'].N()\n",
    "neg_word_count = label_word_fd['neg'].N()\n",
    "total_word_count = pos_word_count + neg_word_count\n",
    " \n",
    "word_scores = {}\n",
    " \n",
    "for word, freq in word_fd.items():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "        (freq, pos_word_count), total_word_count)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "        (freq, neg_word_count), total_word_count)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    " \n",
    "best = sorted(word_scores.items(), key=lambda s: s[1], reverse=True)[:10000]\n",
    "bestwords = set([w for w, s in best])\n",
    " \n",
    "def best_word_feats(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])\n",
    " \n",
    "print ('evaluating best word features')\n",
    "eval_classifier(best_word_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MovieDataModel on Twitter Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "test movie model on twitter test set\n",
      "train on 1500 instances, test on 1800 instances\n",
      "accuracy: 0.5588888888888889\n"
     ]
    }
   ],
   "source": [
    "def movie_tweets_classifier(feats):\n",
    "    # Get the negative reviews for movies    \n",
    "    negids = movie_reviews.fileids('neg')\n",
    "\n",
    "    # Get the positive reviews for movies\n",
    "    posids = movie_reviews.fileids('pos')\n",
    "\n",
    "    # Find the features that most correspond to negative reviews    \n",
    "    negfeats = [(feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "\n",
    "    # Find the features that most correspond to positive reviews\n",
    "    posfeats = [(feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "    # We would only use 1500 instances to train on. The quarter of the reviews left is for testing purposes.\n",
    "    negcutoff = int(len(negfeats)*3/4)\n",
    "    poscutoff = int(len(posfeats)*3/4)\n",
    "\n",
    "    # Construct the training dataset containing 50% positive reviews and 50% negative reviews\n",
    "    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "\n",
    "    # Construct the negative dataset containing 50% positive reviews and 50% negative reviews\n",
    "    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "    # Train a NaiveBayesClassifier\n",
    "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "\n",
    "    twitter_test_set = nltk.classify.apply_features(extract_features,test_tweets)\n",
    "    \n",
    "    print ('test movie model on twitter test set')\n",
    "    print ('train on %d instances, test on %d instances' % (len(trainfeats), len(twitter_test_set)))\n",
    "    print ('accuracy:', nltk.classify.util.accuracy(classifier, twitter_test_set))\n",
    "    \n",
    "\n",
    "movie_tweets_classifier(best_word_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
